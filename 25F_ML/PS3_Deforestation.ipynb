{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e10398f",
   "metadata": {},
   "source": [
    "### 1. Replication of Paper (Linear 2SLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "794a6d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================== FIRST STAGE ===========================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          L_fines_count   R-squared:                       0.031\n",
      "Model:                            OLS   Adj. R-squared:                  0.025\n",
      "Method:                 Least Squares   F-statistic:                     5.111\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):           3.26e-19\n",
      "Time:                        13:25:49   Log-Likelihood:                -22346.\n",
      "No. Observations:                5210   AIC:                         4.476e+04\n",
      "Df Residuals:                    5177   BIC:                         4.498e+04\n",
      "Df Model:                          32                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "const                             0.9993      1.013      0.986      0.324      -0.987       2.986\n",
      "L_deter_cloud                    -9.6628      2.819     -3.428      0.001     -15.188      -4.137\n",
      "L_weather_rain                   -0.0004      0.000     -1.947      0.052      -0.001    2.86e-06\n",
      "L_weather_temp                   -0.5530      0.952     -0.581      0.561      -2.420       1.314\n",
      "prodes_nonobs                     0.0029      0.003      1.138      0.255      -0.002       0.008\n",
      "prodes_cloud                      0.0002      0.000      0.990      0.322      -0.000       0.000\n",
      "priceWgtNdx_brzl_s1_cattle        0.0479      0.122      0.392      0.695      -0.192       0.288\n",
      "L_priceWgtNdx_brzl_s1_cattle     -0.0782      0.116     -0.677      0.499      -0.305       0.148\n",
      "L_priceWgtNdx_brzl_s2_cattle     -0.0918      0.207     -0.443      0.658      -0.498       0.314\n",
      "priceWgtNdx_brzl_s1_sugar         1.3101      3.431      0.382      0.703      -5.417       8.037\n",
      "L_priceWgtNdx_brzl_s1_sugar       1.9101      2.501      0.764      0.445      -2.993       6.814\n",
      "L_priceWgtNdx_brzl_s2_sugar      -1.2767      3.829     -0.333      0.739      -8.782       6.229\n",
      "priceWgtNdx_brzl_s1_corn         -1.7043      1.838     -0.927      0.354      -5.308       1.899\n",
      "L_priceWgtNdx_brzl_s1_corn       -1.9477      2.103     -0.926      0.355      -6.071       2.176\n",
      "L_priceWgtNdx_brzl_s2_corn        2.1276      1.909      1.114      0.265      -1.615       5.870\n",
      "priceWgtNdx_brzl_s1_soybean       0.1857      0.512      0.363      0.717      -0.818       1.189\n",
      "L_priceWgtNdx_brzl_s1_soybean     0.5382      0.545      0.987      0.324      -0.531       1.607\n",
      "L_priceWgtNdx_brzl_s2_soybean    -0.2550      0.461     -0.553      0.580      -1.159       0.649\n",
      "priceWgtNdx_brzl_s1_cassava       3.2047      1.517      2.112      0.035       0.230       6.179\n",
      "L_priceWgtNdx_brzl_s1_cassava     4.0176      1.692      2.375      0.018       0.701       7.334\n",
      "L_priceWgtNdx_brzl_s2_cassava    -2.8534      1.707     -1.671      0.095      -6.200       0.493\n",
      "priceWgtNdx_brzl_s1_rice         -1.3053      2.029     -0.643      0.520      -5.282       2.671\n",
      "L_priceWgtNdx_brzl_s1_rice       -2.2386      2.068     -1.083      0.279      -6.292       1.815\n",
      "L_priceWgtNdx_brzl_s2_rice        3.0754      2.408      1.277      0.202      -1.646       7.797\n",
      "year_2008                         0.9691      1.501      0.646      0.518      -1.973       3.911\n",
      "year_2009                         5.3710      1.332      4.034      0.000       2.761       7.981\n",
      "year_2010                         1.1137      1.414      0.787      0.431      -1.659       3.887\n",
      "year_2011                        -1.8431      1.473     -1.251      0.211      -4.732       1.046\n",
      "year_2012                         0.0394      1.379      0.029      0.977      -2.664       2.743\n",
      "year_2013                        -7.7653      1.403     -5.535      0.000     -10.516      -5.015\n",
      "year_2014                        -3.9441      1.421     -2.776      0.006      -6.730      -1.158\n",
      "year_2015                        -4.7691      1.495     -3.190      0.001      -7.700      -1.838\n",
      "year_2016                         0.8353      1.560      0.535      0.592      -2.224       3.894\n",
      "==============================================================================\n",
      "Omnibus:                     5476.979   Durbin-Watson:                   1.599\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2380810.110\n",
      "Skew:                           4.544   Prob(JB):                         0.00\n",
      "Kurtosis:                     107.330   Cond. No.                     3.19e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.19e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "First-stage F-statistic: 5.110783971586796\n",
      "\n",
      "=========================== SECOND STAGE (TWO-WAY CLUSTERED SE) ===========================\n",
      "                             OLS Regression Results                            \n",
      "===============================================================================\n",
      "Dep. Variable:     prodes_deforest_ihs   R-squared:                       0.228\n",
      "Model:                             OLS   Adj. R-squared:                  0.223\n",
      "Method:                  Least Squares   F-statistic:                     11.31\n",
      "Date:                 Mon, 01 Dec 2025   Prob (F-statistic):           7.77e-42\n",
      "Time:                         13:25:49   Log-Likelihood:                -4475.3\n",
      "No. Observations:                 5210   AIC:                             9017.\n",
      "Df Residuals:                     5177   BIC:                             9233.\n",
      "Df Model:                           32                                         \n",
      "Covariance Type:               cluster                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "const                             0.4345      0.068      6.433      0.000       0.302       0.567\n",
      "xhat                             -0.0503      0.018     -2.825      0.005      -0.085      -0.015\n",
      "L_weather_rain                 -2.12e-05   1.18e-05     -1.800      0.072   -4.43e-05    1.94e-06\n",
      "L_weather_temp                    0.1875      0.053      3.523      0.000       0.083       0.292\n",
      "prodes_nonobs                     0.0004   9.01e-05      4.255      0.000       0.000       0.001\n",
      "prodes_cloud                  -1.712e-05   1.01e-05     -1.703      0.089   -3.69e-05    2.63e-06\n",
      "priceWgtNdx_brzl_s1_cattle       -0.0040      0.005     -0.784      0.433      -0.014       0.006\n",
      "L_priceWgtNdx_brzl_s1_cattle     -0.0201      0.005     -3.771      0.000      -0.031      -0.010\n",
      "L_priceWgtNdx_brzl_s2_cattle      0.0208      0.008      2.584      0.010       0.005       0.037\n",
      "priceWgtNdx_brzl_s1_sugar         0.0670      0.051      1.301      0.194      -0.034       0.168\n",
      "L_priceWgtNdx_brzl_s1_sugar       0.0868      0.053      1.632      0.103      -0.018       0.191\n",
      "L_priceWgtNdx_brzl_s2_sugar      -0.0594      0.066     -0.900      0.369      -0.189       0.070\n",
      "priceWgtNdx_brzl_s1_corn         -0.1399      0.117     -1.199      0.231      -0.369       0.089\n",
      "L_priceWgtNdx_brzl_s1_corn       -0.0689      0.115     -0.599      0.550      -0.295       0.157\n",
      "L_priceWgtNdx_brzl_s2_corn        0.2128      0.086      2.466      0.014       0.043       0.382\n",
      "priceWgtNdx_brzl_s1_soybean       0.0153      0.023      0.655      0.513      -0.031       0.061\n",
      "L_priceWgtNdx_brzl_s1_soybean     0.0308      0.025      1.235      0.217      -0.018       0.080\n",
      "L_priceWgtNdx_brzl_s2_soybean    -0.0169      0.020     -0.845      0.399      -0.056       0.022\n",
      "priceWgtNdx_brzl_s1_cassava       0.0243      0.085      0.286      0.775      -0.142       0.191\n",
      "L_priceWgtNdx_brzl_s1_cassava    -0.0119      0.093     -0.128      0.898      -0.194       0.171\n",
      "L_priceWgtNdx_brzl_s2_cassava    -0.0005      0.075     -0.006      0.995      -0.148       0.147\n",
      "priceWgtNdx_brzl_s1_rice         -0.2598      0.106     -2.451      0.015      -0.468      -0.052\n",
      "L_priceWgtNdx_brzl_s1_rice       -0.3130      0.099     -3.156      0.002      -0.508      -0.118\n",
      "L_priceWgtNdx_brzl_s2_rice        0.5090      0.162      3.137      0.002       0.190       0.828\n",
      "year_2008                         0.2072      0.083      2.490      0.013       0.044       0.371\n",
      "year_2009                        -0.1457      0.103     -1.416      0.157      -0.348       0.056\n",
      "year_2010                        -0.2172      0.089     -2.449      0.015      -0.391      -0.043\n",
      "year_2011                        -0.6738      0.095     -7.078      0.000      -0.861      -0.487\n",
      "year_2012                        -0.7329      0.074     -9.867      0.000      -0.879      -0.587\n",
      "year_2013                        -0.9059      0.148     -6.101      0.000      -1.198      -0.614\n",
      "year_2014                        -0.6945      0.100     -6.915      0.000      -0.892      -0.497\n",
      "year_2015                        -0.7677      0.127     -6.044      0.000      -1.017      -0.518\n",
      "year_2016                        -0.4150      0.080     -5.169      0.000      -0.573      -0.257\n",
      "==============================================================================\n",
      "Omnibus:                      152.621   Durbin-Watson:                   1.859\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              336.112\n",
      "Skew:                           0.156   Prob(JB):                     1.03e-73\n",
      "Kurtosis:                       4.204   Cond. No.                     3.40e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n",
      "[2] The condition number is large, 3.4e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# =========================================================\n",
    "# 0. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"/Users/choi/Desktop/1118_assignment3/data/projectSpecific/panel_forDML.csv\")\n",
    "df = df.sort_values([\"muni_code\", \"year\"]).reset_index(drop=True)\n",
    "\n",
    "# =========================================================\n",
    "# 1. LAGS (Stata L.)\n",
    "# =========================================================\n",
    "group = df.groupby(\"muni_code\")\n",
    "\n",
    "df[\"L_fines_count\"]   = group[\"fines_count\"].shift(1)\n",
    "df[\"L_deter_cloud\"]   = group[\"deter_cloud\"].shift(1)\n",
    "df[\"L_weather_rain\"]  = group[\"weather_rain\"].shift(1)\n",
    "df[\"L_weather_temp\"]  = group[\"weather_temp\"].shift(1)\n",
    "\n",
    "price_vars = [\n",
    "    \"priceWgtNdx_brzl_s1_cattle\", \"priceWgtNdx_brzl_s1_sugar\",\n",
    "    \"priceWgtNdx_brzl_s1_corn\",   \"priceWgtNdx_brzl_s1_soybean\",\n",
    "    \"priceWgtNdx_brzl_s1_cassava\",\"priceWgtNdx_brzl_s1_rice\"\n",
    "]\n",
    "\n",
    "for v in price_vars:\n",
    "    df[f\"L_{v}\"] = group[v].shift(1)\n",
    "    df[f\"L_{v.replace('s1','s2')}\"] = group[v.replace(\"s1\",\"s2\")].shift(1)\n",
    "\n",
    "# =========================================================\n",
    "# 2. SAMPLE SELECTION — FILTER FIRST (CRUCIAL)\n",
    "# =========================================================\n",
    "df = df[(df[\"year\"] >= 2007) & (df[\"year\"] <= 2016)]\n",
    "df = df.dropna(subset=[\"L_fines_count\", \"L_deter_cloud\"])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# =========================================================\n",
    "# 3. WITHIN TRANSFORMATION\n",
    "# =========================================================\n",
    "def within(df, group, cols):\n",
    "    return df[cols] - df.groupby(group)[cols].transform(\"mean\")\n",
    "\n",
    "y_var = \"prodes_deforest_ihs\"\n",
    "# y_var = \"prodes_deforest_log\"\n",
    "# y_var = \"prodes_deforest_dma_100\"\n",
    "# y_var = \"prodes_deforest_dbm\"\n",
    "\n",
    "endog = \"L_fines_count\"\n",
    "instr = \"L_deter_cloud\"\n",
    "\n",
    "controls = [\"L_weather_rain\", \"L_weather_temp\", \"prodes_nonobs\", \"prodes_cloud\"]\n",
    "for v in price_vars:\n",
    "    controls += [v, f\"L_{v}\", f\"L_{v.replace('s1','s2')}\"]\n",
    "\n",
    "demean_vars = [y_var, endog, instr] + controls\n",
    "df_w = within(df, \"muni_code\", demean_vars)\n",
    "\n",
    "# =========================================================\n",
    "# 4. YEAR FE\n",
    "# =========================================================\n",
    "year_fe = pd.get_dummies(df[\"year\"], prefix=\"year\", drop_first=True)\n",
    "X_controls = pd.concat([df_w[controls], year_fe], axis=1)\n",
    "\n",
    "# =========================================================\n",
    "# 5. FIRST STAGE\n",
    "# =========================================================\n",
    "Z = df_w[[instr]]\n",
    "X1 = pd.concat([Z, X_controls], axis=1)\n",
    "X1 = X1.copy()          # ensure no view\n",
    "X1.insert(0, \"const\", 1.0)\n",
    "\n",
    "\n",
    "X1 = X1.apply(pd.to_numeric, errors=\"coerce\")\n",
    "y1 = pd.to_numeric(df_w[endog], errors=\"coerce\")\n",
    "\n",
    "mask = X1.notnull().all(axis=1) & y1.notnull()\n",
    "X1 = X1.loc[mask]\n",
    "y1 = y1.loc[mask]\n",
    "\n",
    "X1 = X1.astype(float)\n",
    "y1 = y1.astype(float)\n",
    "\n",
    "\n",
    "fs = OLS(y1, X1).fit()\n",
    "print(\"\\n=========================== FIRST STAGE ===========================\")\n",
    "print(fs.summary())\n",
    "print(\"\\nFirst-stage F-statistic:\", fs.fvalue)\n",
    "\n",
    "x_hat = fs.fittedvalues\n",
    "\n",
    "# =========================================================\n",
    "# 6. SECOND STAGE\n",
    "# =========================================================\n",
    "X2 = pd.concat([x_hat.rename(\"xhat\"), X_controls.loc[mask]], axis=1)\n",
    "X2 = X2.copy()\n",
    "X2.insert(0, \"const\", 1.0)\n",
    "\n",
    "\n",
    "y2 = df_w[y_var].loc[mask]\n",
    "\n",
    "X2 = X2.astype(float)\n",
    "y2 = y2.astype(float)\n",
    "\n",
    "\n",
    "ss = OLS(y2, X2).fit()\n",
    "# print(\"\\n=========================== SECOND STAGE (NO CLUSTER) ===========================\")\n",
    "# print(ss.summary())\n",
    "\n",
    "# =========================================================\n",
    "# 7. TWO-WAY CLUSTERED SE\n",
    "# =========================================================\n",
    "\n",
    "# Convert cluster variable to numeric codes\n",
    "df[\"cl_microYear_code\"] = df[\"cl_microYear\"].astype(\"category\").cat.codes\n",
    "\n",
    "clusters = df.loc[mask, [\"muni_code\", \"cl_microYear_code\"]]\n",
    "\n",
    "ss_cluster = ss.get_robustcov_results(\n",
    "    cov_type=\"cluster\",\n",
    "    groups=clusters,\n",
    ")\n",
    "\n",
    "print(\"\\n=========================== SECOND STAGE (TWO-WAY CLUSTERED SE) ===========================\")\n",
    "print(ss_cluster.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a685b2",
   "metadata": {},
   "source": [
    "### 2. Double‐Selection LASSO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "06a28be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# =========================================================\n",
    "# 0. LOAD AND PREPARE DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"/Users/choi/Desktop/1118_assignment3/data/projectSpecific/panel_forDML.csv\")\n",
    "df = df.sort_values([\"muni_code\", \"year\"]).reset_index(drop=True)\n",
    "\n",
    "# lags\n",
    "group = df.groupby(\"muni_code\")\n",
    "df[\"L_fines_count\"]   = group[\"fines_count\"].shift(1)\n",
    "df[\"L_deter_cloud\"]   = group[\"deter_cloud\"].shift(1)\n",
    "df[\"L_weather_rain\"]  = group[\"weather_rain\"].shift(1)\n",
    "df[\"L_weather_temp\"]  = group[\"weather_temp\"].shift(1)\n",
    "\n",
    "price_vars = [\n",
    "    \"priceWgtNdx_brzl_s1_cattle\",\"priceWgtNdx_brzl_s1_sugar\",\n",
    "    \"priceWgtNdx_brzl_s1_corn\",\"priceWgtNdx_brzl_s1_soybean\",\n",
    "    \"priceWgtNdx_brzl_s1_cassava\",\"priceWgtNdx_brzl_s1_rice\"\n",
    "]\n",
    "for v in price_vars:\n",
    "    df[f\"L_{v}\"] = group[v].shift(1)\n",
    "    df[f\"L_{v.replace('s1','s2')}\"] = group[v.replace(\"s1\",\"s2\")].shift(1)\n",
    "\n",
    "# sample: 2007–2016\n",
    "df = df[(df[\"year\"] >= 2007) & (df[\"year\"] <= 2016)]\n",
    "df = df.dropna(subset=[\"L_fines_count\",\"L_deter_cloud\"]).reset_index(drop=True)\n",
    "\n",
    "# =========================================================\n",
    "# 1. MUNICIPALITY FE (WITHIN TRANSFORMATION)\n",
    "# =========================================================\n",
    "def within(df, group, cols):\n",
    "    return df[cols] - df.groupby(group)[cols].transform(\"mean\")\n",
    "\n",
    "# y_var = \"prodes_deforest_ihs\"\n",
    "# y_var = \"prodes_deforest_log\"\n",
    "# y_var = \"prodes_deforest_dma_100\"\n",
    "y_var = \"prodes_deforest_dbm\"\n",
    "\n",
    "endog = \"L_fines_count\"        # y1 in your notes\n",
    "instr = \"L_deter_cloud\"        # Z\n",
    "\n",
    "controls = [\"L_weather_rain\",\"L_weather_temp\",\"prodes_nonobs\",\"prodes_cloud\"]\n",
    "for v in price_vars:\n",
    "    controls += [v, f\"L_{v}\", f\"L_{v.replace('s1','s2')}\"]\n",
    "\n",
    "demean_vars = [y_var, endog, instr] + controls\n",
    "df_w = within(df, \"muni_code\", demean_vars)\n",
    "\n",
    "# =========================================================\n",
    "# 2. YEAR FIXED EFFECTS\n",
    "# =========================================================\n",
    "year_fe = pd.get_dummies(df[\"year\"], prefix=\"year\", drop_first=True)\n",
    "year_fe = year_fe.astype(float)\n",
    "X_controls = pd.concat([df_w[controls], year_fe], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "201afdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected controls by double LASSO:\n",
      "['L_weather_rain', 'L_weather_temp', 'prodes_nonobs', 'prodes_cloud', 'priceWgtNdx_brzl_s1_cattle', 'L_priceWgtNdx_brzl_s1_cattle', 'L_priceWgtNdx_brzl_s1_sugar', 'priceWgtNdx_brzl_s1_corn', 'L_priceWgtNdx_brzl_s1_corn', 'L_priceWgtNdx_brzl_s1_soybean', 'priceWgtNdx_brzl_s1_cassava', 'L_priceWgtNdx_brzl_s1_cassava', 'L_priceWgtNdx_brzl_s2_cassava', 'priceWgtNdx_brzl_s1_rice', 'L_priceWgtNdx_brzl_s1_rice', 'L_priceWgtNdx_brzl_s2_rice', 'year_2008', 'year_2009', 'year_2010', 'year_2011', 'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016']\n",
      "\n",
      "========== FIRST STAGE (OLS): L_fines_count ~ L_deter_cloud + selected X ==========\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.030\n",
      "Model:                            OLS   Adj. R-squared:                  0.025\n",
      "Method:                 Least Squares   F-statistic:                     6.230\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):           3.21e-21\n",
      "Time:                        14:27:12   Log-Likelihood:                -22347.\n",
      "No. Observations:                5210   AIC:                         4.475e+04\n",
      "Df Residuals:                    5183   BIC:                         4.493e+04\n",
      "Df Model:                          26                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "const                             0.8781      0.996      0.882      0.378      -1.075       2.831\n",
      "L_deter_cloud                    -9.8466      2.810     -3.504      0.000     -15.355      -4.338\n",
      "L_weather_rain                   -0.0004      0.000     -1.926      0.054      -0.001    7.18e-06\n",
      "L_weather_temp                   -0.5206      0.948     -0.549      0.583      -2.379       1.338\n",
      "prodes_nonobs                     0.0029      0.003      1.144      0.253      -0.002       0.008\n",
      "prodes_cloud                      0.0002      0.000      0.958      0.338      -0.000       0.000\n",
      "priceWgtNdx_brzl_s1_cattle        0.0062      0.073      0.085      0.933      -0.137       0.150\n",
      "L_priceWgtNdx_brzl_s1_cattle     -0.1260      0.065     -1.927      0.054      -0.254       0.002\n",
      "L_priceWgtNdx_brzl_s1_sugar       1.5253      1.478      1.032      0.302      -1.372       4.422\n",
      "priceWgtNdx_brzl_s1_corn         -0.5531      1.347     -0.410      0.681      -3.195       2.088\n",
      "L_priceWgtNdx_brzl_s1_corn       -0.3546      1.462     -0.242      0.808      -3.222       2.512\n",
      "L_priceWgtNdx_brzl_s1_soybean     0.2654      0.361      0.735      0.462      -0.442       0.973\n",
      "priceWgtNdx_brzl_s1_cassava       3.2618      1.510      2.161      0.031       0.302       6.221\n",
      "L_priceWgtNdx_brzl_s1_cassava     3.9948      1.690      2.363      0.018       0.681       7.309\n",
      "L_priceWgtNdx_brzl_s2_cassava    -2.8849      1.706     -1.691      0.091      -6.229       0.459\n",
      "priceWgtNdx_brzl_s1_rice         -1.1241      2.016     -0.558      0.577      -5.076       2.828\n",
      "L_priceWgtNdx_brzl_s1_rice       -2.5286      2.033     -1.244      0.214      -6.515       1.458\n",
      "L_priceWgtNdx_brzl_s2_rice        3.0433      2.401      1.267      0.205      -1.664       7.751\n",
      "year_2008                         1.3203      1.457      0.906      0.365      -1.537       4.177\n",
      "year_2009                         5.2431      1.316      3.985      0.000       2.664       7.823\n",
      "year_2010                         1.3456      1.378      0.977      0.329      -1.355       4.047\n",
      "year_2011                        -1.7939      1.463     -1.226      0.220      -4.662       1.074\n",
      "year_2012                         0.1182      1.370      0.086      0.931      -2.568       2.804\n",
      "year_2013                        -7.4448      1.302     -5.719      0.000      -9.997      -4.893\n",
      "year_2014                        -3.8227      1.330     -2.873      0.004      -6.431      -1.215\n",
      "year_2015                        -4.6250      1.486     -3.112      0.002      -7.538      -1.712\n",
      "year_2016                         0.8785      1.547      0.568      0.570      -2.154       3.911\n",
      "==============================================================================\n",
      "Omnibus:                     5476.189   Durbin-Watson:                   1.600\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2379973.942\n",
      "Skew:                           4.543   Prob(JB):                         0.00\n",
      "Kurtosis:                     107.311   Cond. No.                     2.25e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.25e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "========== SECOND STAGE (Two-Way Clustered SE) ==========\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.142\n",
      "Model:                            OLS   Adj. R-squared:                  0.137\n",
      "Method:                 Least Squares   F-statistic:                     10.90\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):           8.17e-35\n",
      "Time:                        14:27:12   Log-Likelihood:                -5318.3\n",
      "No. Observations:                5210   AIC:                         1.069e+04\n",
      "Df Residuals:                    5183   BIC:                         1.087e+04\n",
      "Df Model:                          26                                         \n",
      "Covariance Type:              cluster                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "const                             0.2850      0.055      5.219      0.000       0.178       0.392\n",
      "fines_hat_ds                     -0.0456      0.020     -2.326      0.020      -0.084      -0.007\n",
      "L_weather_rain                -1.864e-05   1.12e-05     -1.671      0.095   -4.06e-05    3.28e-06\n",
      "L_weather_temp                   -0.0067      0.048     -0.139      0.889      -0.101       0.088\n",
      "prodes_nonobs                     0.0002      0.000      1.993      0.047    3.33e-06       0.000\n",
      "prodes_cloud                  -9.241e-06   9.26e-06     -0.997      0.319   -2.74e-05    8.96e-06\n",
      "priceWgtNdx_brzl_s1_cattle        0.0068      0.003      2.253      0.025       0.001       0.013\n",
      "L_priceWgtNdx_brzl_s1_cattle     -0.0086      0.004     -2.392      0.017      -0.016      -0.002\n",
      "L_priceWgtNdx_brzl_s1_sugar       0.1052      0.041      2.557      0.011       0.024       0.186\n",
      "priceWgtNdx_brzl_s1_corn          0.0964      0.088      1.093      0.275      -0.077       0.270\n",
      "L_priceWgtNdx_brzl_s1_corn        0.1072      0.123      0.872      0.384      -0.134       0.349\n",
      "L_priceWgtNdx_brzl_s1_soybean     0.0258      0.016      1.604      0.109      -0.006       0.057\n",
      "priceWgtNdx_brzl_s1_cassava      -0.1755      0.115     -1.519      0.129      -0.402       0.051\n",
      "L_priceWgtNdx_brzl_s1_cassava    -0.1386      0.121     -1.145      0.253      -0.376       0.099\n",
      "L_priceWgtNdx_brzl_s2_cassava     0.1475      0.097      1.527      0.127      -0.042       0.337\n",
      "priceWgtNdx_brzl_s1_rice         -0.3468      0.180     -1.922      0.055      -0.701       0.008\n",
      "L_priceWgtNdx_brzl_s1_rice       -0.4818      0.160     -3.004      0.003      -0.797      -0.167\n",
      "L_priceWgtNdx_brzl_s2_rice        0.8803      0.417      2.112      0.035       0.061       1.699\n",
      "year_2008                         0.2638      0.090      2.947      0.003       0.088       0.440\n",
      "year_2009                        -0.0650      0.116     -0.559      0.576      -0.293       0.163\n",
      "year_2010                         0.0227      0.102      0.223      0.824      -0.177       0.223\n",
      "year_2011                        -0.4453      0.078     -5.732      0.000      -0.598      -0.293\n",
      "year_2012                        -0.4503      0.066     -6.820      0.000      -0.580      -0.321\n",
      "year_2013                        -0.7361      0.146     -5.057      0.000      -1.022      -0.450\n",
      "year_2014                        -0.5260      0.086     -6.125      0.000      -0.695      -0.357\n",
      "year_2015                        -0.6336      0.116     -5.483      0.000      -0.861      -0.407\n",
      "year_2016                        -0.2800      0.075     -3.747      0.000      -0.427      -0.133\n",
      "==============================================================================\n",
      "Omnibus:                     6662.948   Durbin-Watson:                   2.137\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2195403.607\n",
      "Skew:                           6.778   Prob(JB):                         0.00\n",
      "Kurtosis:                     102.646   Cond. No.                     2.84e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n",
      "[2] The condition number is large, 2.84e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# =========================================================\n",
    "# 3. DOUBLE-SELECTION LASSO TO CHOOSE CONTROLS (LINEAR)\n",
    "# =========================================================\n",
    "\n",
    "# Outcome, endogenous, instrument (demeaned)\n",
    "y_ds = df_w[y_var].values          # prodes_deforest_ihs (demeaned) \n",
    "D_ds = df_w[endog].values          # L_fines_count (demeaned) \n",
    "Z_ds = df_w[instr].values          # L_deter_cloud (demeaned) \n",
    "\n",
    "# Controls for LASSO: ONLY X, NO instrument\n",
    "X_lasso = X_controls.copy()        # DataFrame of controls (demeaned + year FE)\n",
    "control_names = X_lasso.columns\n",
    "\n",
    "# Standardize controls for LASSO\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_lasso.values)\n",
    "\n",
    "# ---- LASSO 1: deforest ~ controls ----\n",
    "lasso_y = LassoCV(cv=5, random_state=123).fit(X_std, y_ds)\n",
    "sel_y_idx = np.where(lasso_y.coef_ != 0)[0]\n",
    "\n",
    "# ---- LASSO 2: fines_count ~ controls ----\n",
    "lasso_D = LassoCV(cv=5, random_state=123).fit(X_std, D_ds)\n",
    "sel_D_idx = np.where(lasso_D.coef_ != 0)[0]\n",
    "\n",
    "# ---- Union of selected controls ----\n",
    "sel_union_idx = np.unique(np.concatenate([sel_y_idx, sel_D_idx]))\n",
    "selected_cols = control_names[sel_union_idx]\n",
    "\n",
    "print(\"Selected controls by double LASSO:\")\n",
    "print(list(selected_cols))\n",
    "\n",
    "X_sel = X_controls[selected_cols]      # DataFrame of selected controls (demeaned)\n",
    "\n",
    "# =========================================================\n",
    "# 4. LINEAR 2SLS WITH SELECTED CONTROLS\n",
    "# =========================================================\n",
    "print(\"\\n========== FIRST STAGE (OLS): L_fines_count ~ L_deter_cloud + selected X ==========\")\n",
    "\n",
    "# First stage: L_fines_count ~ L_deter_cloud + selected_controls\n",
    "X_first = pd.concat([\n",
    "    df_w[[instr]],   # L_deter_cloud (demeaned instrument)\n",
    "    X_sel            # selected controls\n",
    "], axis=1)\n",
    "X_first = add_constant(X_first)       # add intercept\n",
    "\n",
    "first_stage = OLS(D_ds, X_first).fit()\n",
    "print(first_stage.summary())\n",
    "\n",
    "# Predicted fines (linear first stage)\n",
    "fines_hat_ds = first_stage.fittedvalues\n",
    "\n",
    "# print(\"\\n========== SECOND STAGE (OLS): deforest ~ fines_hat_ds + selected X ==========\")\n",
    "\n",
    "# Second stage: prodes_deforest_ihs ~ fines_hat_ds + selected_controls\n",
    "X_second = pd.concat([\n",
    "    pd.Series(fines_hat_ds, name=\"fines_hat_ds\"),\n",
    "    X_sel\n",
    "], axis=1)\n",
    "X_second = add_constant(X_second)\n",
    "\n",
    "second_stage = OLS(y_ds, X_second).fit()\n",
    "# print(second_stage.summary())\n",
    "\n",
    "# clustering\n",
    "clusters = df.loc[X_second.index, [\"muni_code\", \"cl_microYear\"]].copy()\n",
    "clusters[\"muni_code\"] = clusters[\"muni_code\"].astype(int)\n",
    "clusters[\"cl_microYear\"] = clusters[\"cl_microYear\"].astype(\"category\").cat.codes.astype(int)\n",
    "\n",
    "ss_cluster = second_stage.get_robustcov_results(\n",
    "    cov_type=\"cluster\",\n",
    "    groups=clusters\n",
    ")\n",
    "\n",
    "print(\"\\n========== SECOND STAGE (Two-Way Clustered SE) ==========\")\n",
    "print(ss_cluster.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00ac01",
   "metadata": {},
   "source": [
    "### 3. Partially Linear DML-IV (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b5eb46c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                     1.347\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):              0.246\n",
      "Time:                        20:42:31   Log-Likelihood:                -22376.\n",
      "No. Observations:                5210   AIC:                         4.476e+04\n",
      "Df Residuals:                    5208   BIC:                         4.477e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0783      0.246     -0.319      0.750      -0.560       0.404\n",
      "x1            -2.2234      1.915     -1.161      0.246      -5.978       1.532\n",
      "==============================================================================\n",
      "Omnibus:                     5438.565   Durbin-Watson:                   1.622\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2282035.379\n",
      "Skew:                           4.496   Prob(JB):                         0.00\n",
      "Kurtosis:                     105.134   Cond. No.                         7.79\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "========== DML-IV with Clustered SE ==========\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.001\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                    0.7913\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):              0.374\n",
      "Time:                        20:42:31   Log-Likelihood:                -4990.0\n",
      "No. Observations:                5210   AIC:                             9984.\n",
      "Df Residuals:                    5208   BIC:                             9997.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:              cluster                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0104      0.010     -1.041      0.299      -0.030       0.009\n",
      "x1            -0.0541      0.061     -0.890      0.374      -0.173       0.065\n",
      "==============================================================================\n",
      "Omnibus:                     6357.942   Durbin-Watson:                   2.225\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2132867.717\n",
      "Skew:                           6.172   Prob(JB):                         0.00\n",
      "Kurtosis:                     101.350   Cond. No.                         3.53\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# =========================================================\n",
    "# 3. PARTIALLY LINEAR DML (ML TO PARTIAL OUT CONTROLS)\n",
    "# =========================================================\n",
    "\n",
    "# variables (demeaned)\n",
    "Y = df_w[y_var].values           # deforest\n",
    "D = df_w[endog].values           # fines\n",
    "Z = df_w[instr].values           # cloud\n",
    "X = X_controls.values            # nonlinear controls (demeaned + year FE)\n",
    "\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "# storage for cross-fitted residuals\n",
    "Y_res = np.zeros_like(Y)\n",
    "D_res = np.zeros_like(D)\n",
    "\n",
    "# ML model for nonlinear controls\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    min_samples_leaf=5,\n",
    "    max_depth=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # --- ML: predict E[Y|X] ---\n",
    "    rf.fit(X[train_idx], Y[train_idx])\n",
    "    Y_res[test_idx] = Y[test_idx] - rf.predict(X[test_idx])\n",
    "\n",
    "    # --- ML: predict E[D|X] ---\n",
    "    rf.fit(X[train_idx], D[train_idx])\n",
    "    D_res[test_idx] = D[test_idx] - rf.predict(X[test_idx])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. DML-IV: LINEAR IV ON RESIDUALS\n",
    "# =========================================================\n",
    "\n",
    "# FIRST STAGE: D_res ~ Z_res\n",
    "X_first = add_constant(Z)\n",
    "first_stage = OLS(D_res, X_first).fit()\n",
    "D_hat = first_stage.fittedvalues\n",
    "\n",
    "print(first_stage.summary())\n",
    "\n",
    "# SECOND STAGE: Y_res ~ D_hat\n",
    "X_second = add_constant(D_hat)\n",
    "second_stage = OLS(Y_res, X_second).fit()\n",
    "\n",
    "# print(second_stage.summary())\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. OLS Cluster SE (NOT ROBUST TO DML?)\n",
    "# =========================================================\n",
    "\n",
    "# If you want cluster-robust SEs, same code as before:\n",
    "clusters = df[[\"muni_code\",\"cl_microYear\"]].copy()\n",
    "clusters[\"muni_code\"] = clusters[\"muni_code\"].astype(int)\n",
    "clusters[\"cl_microYear\"] = clusters[\"cl_microYear\"].astype(\"category\").cat.codes.astype(int)\n",
    "\n",
    "ss_cluster = second_stage.get_robustcov_results( \n",
    "    cov_type=\"cluster\",\n",
    "    groups=clusters\n",
    ")\n",
    "print(\"\\n========== DML-IV with Clustered SE ==========\")\n",
    "print(ss_cluster.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6cbdf1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_hat (DML-IV): -0.054059\n",
      "Two-Way Clustered DML SE: 0.075959\n",
      "95% CI (two-way cluster): (-0.202935, 0.094817)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# from second-stage OLS on residuals\n",
    "theta_hat = second_stage.params[1]\n",
    "theta_hat\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. Required objects\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# cluster IDs as 1D numpy arrays\n",
    "cluster1 = np.asarray(clusters[\"muni_code\"].values)\n",
    "cluster2 = np.asarray(clusters[\"cl_microYear\"].values)\n",
    "\n",
    "# orthogonal scores: ψ_i = Z_res * (Y_res - theta_hat * D_res)\n",
    "psi = Z * (Y_res - theta_hat * D_res)\n",
    "\n",
    "# Jacobian:  J = -(1/n) * Σ Z_res * D_res\n",
    "J = -np.mean(Z * D_res)\n",
    "\n",
    "n = len(psi)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Compute cluster sums of ψ_i\n",
    "# ------------------------------------------------------------\n",
    "def cluster_sum(psi_vec, cluster_id_vec):\n",
    "    cluster_id_vec = np.asarray(cluster_id_vec)\n",
    "    out = {}\n",
    "    for cid in np.unique(cluster_id_vec):\n",
    "        mask = (cluster_id_vec == cid)\n",
    "        out[cid] = psi_vec[mask].sum()\n",
    "    return out\n",
    "\n",
    "# cluster 1 sums\n",
    "C1 = cluster_sum(psi, cluster1)\n",
    "# cluster 2 sums\n",
    "C2 = cluster_sum(psi, cluster2)\n",
    "\n",
    "# intersection clusters: create a single combined cluster label\n",
    "cluster12 = np.array([f\"{a}_{b}\" for a, b in zip(cluster1, cluster2)])\n",
    "C12 = cluster_sum(psi, cluster12)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Variance components\n",
    "# ------------------------------------------------------------\n",
    "V1  = sum(v**2 for v in C1.values())\n",
    "V2  = sum(v**2 for v in C2.values())\n",
    "V12 = sum(v**2 for v in C12.values())\n",
    "\n",
    "# Two-way cluster variance (Cameron–Gelbach–Miller)\n",
    "cluster_var = (V1 + V2 - V12) / (n**2)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Delta method for θ (Jacobian scaling)\n",
    "# ------------------------------------------------------------\n",
    "Var_theta = cluster_var / (J**2)\n",
    "se_dml_tw = np.sqrt(Var_theta)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. CI\n",
    "# ------------------------------------------------------------\n",
    "z_95 = norm.ppf(0.975)\n",
    "ci_low = theta_hat - z_95 * se_dml_tw\n",
    "ci_high = theta_hat + z_95 * se_dml_tw\n",
    "\n",
    "print(f\"Theta_hat (DML-IV): {theta_hat:.6f}\")\n",
    "print(f\"Two-Way Clustered DML SE: {se_dml_tw:.6f}\")\n",
    "print(f\"95% CI (two-way cluster): ({ci_low:.6f}, {ci_high:.6f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85beab3b",
   "metadata": {},
   "source": [
    "### 4. Partially Linear DML-IV (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4e2b11d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                    0.1916\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):              0.662\n",
      "Time:                        20:42:35   Log-Likelihood:                -22403.\n",
      "No. Observations:                5210   AIC:                         4.481e+04\n",
      "Df Residuals:                    5208   BIC:                         4.482e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0643      0.247      0.260      0.795      -0.420       0.549\n",
      "x1            -0.8428      1.926     -0.438      0.662      -4.618       2.932\n",
      "==============================================================================\n",
      "Omnibus:                     5454.831   Durbin-Watson:                   1.603\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2316127.969\n",
      "Skew:                           4.517   Prob(JB):                         0.00\n",
      "Kurtosis:                     105.896   Cond. No.                         7.79\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "========== DML-IV (Shallow NN) with Clustered SE ==========\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.005\n",
      "Model:                            OLS   Adj. R-squared:                  0.005\n",
      "Method:                 Least Squares   F-statistic:                     4.626\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):             0.0320\n",
      "Time:                        20:42:35   Log-Likelihood:                -5417.2\n",
      "No. Observations:                5210   AIC:                         1.084e+04\n",
      "Df Residuals:                    5208   BIC:                         1.085e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:              cluster                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0305      0.021      1.437      0.151      -0.011       0.072\n",
      "x1            -0.4368      0.203     -2.151      0.032      -0.836      -0.038\n",
      "==============================================================================\n",
      "Omnibus:                     6608.652   Durbin-Watson:                   2.139\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2126707.854\n",
      "Skew:                           6.679   Prob(JB):                         0.00\n",
      "Kurtosis:                     101.073   Cond. No.                         9.28\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.api import OLS, add_constant\n",
    "import numpy as np\n",
    "\n",
    "# =========================================================\n",
    "# 3. PARTIALLY LINEAR DML (ML TO PARTIAL OUT CONTROLS)\n",
    "# =========================================================\n",
    "\n",
    "# variables (demeaned)\n",
    "Y = df_w[y_var].values           # deforest\n",
    "D = df_w[endog].values           # fines\n",
    "Z = df_w[instr].values           # cloud instrument\n",
    "X = X_controls.values            # nonlinear controls\n",
    "\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "# storage for cross-fitted residuals\n",
    "Y_res = np.zeros_like(Y)\n",
    "D_res = np.zeros_like(D)\n",
    "\n",
    "# ----- SIMPLE SHALLOW NN -----\n",
    "nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(10,),   # 1 hidden layer, 10 neurons\n",
    "    activation='tanh',          # smooth + stable\n",
    "    # solver='lbfgs',             # deterministic, best for small nets\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # --- ML: predict E[Y | X] ---\n",
    "    nn.fit(X[train_idx], Y[train_idx])\n",
    "    Y_res[test_idx] = Y[test_idx] - nn.predict(X[test_idx])\n",
    "\n",
    "    # --- ML: predict E[D | X] ---\n",
    "    nn.fit(X[train_idx], D[train_idx])\n",
    "    D_res[test_idx] = D[test_idx] - nn.predict(X[test_idx])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. DML-IV: LINEAR IV ON RESIDUALs\n",
    "# =========================================================\n",
    "\n",
    "# FIRST STAGE: D_res ~ Z\n",
    "X_first = add_constant(Z)\n",
    "first_stage = OLS(D_res, X_first).fit()\n",
    "D_hat = first_stage.fittedvalues\n",
    "print(first_stage.summary())\n",
    "\n",
    "# SECOND STAGE: Y_res ~ D_hat\n",
    "X_second = add_constant(D_hat)\n",
    "second_stage = OLS(Y_res, X_second).fit()\n",
    "# print(second_stage.summary())\n",
    "\n",
    "# =========================================================\n",
    "# 5. Clustered SE\n",
    "# =========================================================\n",
    "\n",
    "clusters = df[[\"muni_code\",\"cl_microYear\"]].copy()\n",
    "clusters[\"muni_code\"] = clusters[\"muni_code\"].astype(int)\n",
    "clusters[\"cl_microYear\"] = clusters[\"cl_microYear\"].astype(\"category\").cat.codes.astype(int)\n",
    "\n",
    "ss_cluster = second_stage.get_robustcov_results(\n",
    "    cov_type=\"cluster\",\n",
    "    groups=clusters\n",
    ")\n",
    "\n",
    "print(\"\\n========== DML-IV (Shallow NN) with Clustered SE ==========\")\n",
    "print(ss_cluster.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c1e339f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_hat (DML-IV): -0.436789\n",
      "Two-Way Clustered DML SE: 1.041206\n",
      "95% CI (two-way cluster): (-2.477515, 1.603936)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# from second-stage OLS on residuals\n",
    "theta_hat = second_stage.params[1]\n",
    "theta_hat\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. Required objects\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# cluster IDs as 1D numpy arrays\n",
    "cluster1 = np.asarray(clusters[\"muni_code\"].values)\n",
    "cluster2 = np.asarray(clusters[\"cl_microYear\"].values)\n",
    "\n",
    "# orthogonal scores: ψ_i = Z_res * (Y_res - theta_hat * D_res)\n",
    "psi = Z * (Y_res - theta_hat * D_res)\n",
    "\n",
    "# Jacobian:  J = -(1/n) * Σ Z_res * D_res\n",
    "J = -np.mean(Z * D_res)\n",
    "\n",
    "n = len(psi)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Compute cluster sums of ψ_i\n",
    "# ------------------------------------------------------------\n",
    "def cluster_sum(psi_vec, cluster_id_vec):\n",
    "    cluster_id_vec = np.asarray(cluster_id_vec)\n",
    "    out = {}\n",
    "    for cid in np.unique(cluster_id_vec):\n",
    "        mask = (cluster_id_vec == cid)\n",
    "        out[cid] = psi_vec[mask].sum()\n",
    "    return out\n",
    "\n",
    "# cluster 1 sums\n",
    "C1 = cluster_sum(psi, cluster1)\n",
    "# cluster 2 sums\n",
    "C2 = cluster_sum(psi, cluster2)\n",
    "\n",
    "# intersection clusters: create a single combined cluster label\n",
    "cluster12 = np.array([f\"{a}_{b}\" for a, b in zip(cluster1, cluster2)])\n",
    "C12 = cluster_sum(psi, cluster12)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Variance components\n",
    "# ------------------------------------------------------------\n",
    "V1  = sum(v**2 for v in C1.values())\n",
    "V2  = sum(v**2 for v in C2.values())\n",
    "V12 = sum(v**2 for v in C12.values())\n",
    "\n",
    "# Two-way cluster variance (Cameron–Gelbach–Miller)\n",
    "cluster_var = (V1 + V2 - V12) / (n**2)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Delta method for θ (Jacobian scaling)\n",
    "# ------------------------------------------------------------\n",
    "Var_theta = cluster_var / (J**2)\n",
    "se_dml_tw = np.sqrt(Var_theta)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. CI\n",
    "# ------------------------------------------------------------\n",
    "z_95 = norm.ppf(0.975)\n",
    "ci_low = theta_hat - z_95 * se_dml_tw\n",
    "ci_high = theta_hat + z_95 * se_dml_tw\n",
    "\n",
    "print(f\"Theta_hat (DML-IV): {theta_hat:.6f}\")\n",
    "print(f\"Two-Way Clustered DML SE: {se_dml_tw:.6f}\")\n",
    "print(f\"95% CI (two-way cluster): ({ci_low:.6f}, {ci_high:.6f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7214a",
   "metadata": {},
   "source": [
    "### 5. Partially Linear DML-IV (XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "80bb0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                     2.282\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):              0.131\n",
      "Time:                        16:50:42   Log-Likelihood:                -22481.\n",
      "No. Observations:                5210   AIC:                         4.497e+04\n",
      "Df Residuals:                    5208   BIC:                         4.498e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0342      0.251     -0.137      0.891      -0.526       0.458\n",
      "x1            -2.9526      1.954     -1.511      0.131      -6.784       0.879\n",
      "==============================================================================\n",
      "Omnibus:                     5280.084   Durbin-Watson:                   1.657\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1948771.505\n",
      "Skew:                           4.294   Prob(JB):                         0.00\n",
      "Kurtosis:                      97.357   Cond. No.                         7.79\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "========== DML-IV (Shallow NN) with Clustered SE ==========\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.001\n",
      "Model:                            OLS   Adj. R-squared:                  0.001\n",
      "Method:                 Least Squares   F-statistic:                     1.379\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):              0.241\n",
      "Time:                        16:50:42   Log-Likelihood:                -5116.9\n",
      "No. Observations:                5210   AIC:                         1.024e+04\n",
      "Df Residuals:                    5208   BIC:                         1.025e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:              cluster                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0022      0.011     -0.206      0.837      -0.023       0.019\n",
      "x1            -0.0575      0.049     -1.174      0.241      -0.154       0.039\n",
      "==============================================================================\n",
      "Omnibus:                     6276.464   Durbin-Watson:                   2.227\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1882223.019\n",
      "Skew:                           6.065   Prob(JB):                         0.00\n",
      "Kurtosis:                      95.322   Cond. No.                         2.64\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# =========================================================\n",
    "# 3. PARTIALLY LINEAR DML (ML TO PARTIAL OUT CONTROLS)\n",
    "# =========================================================\n",
    "\n",
    "# variables (demeaned)\n",
    "Y = df_w[y_var].values           # deforest\n",
    "D = df_w[endog].values           # fines\n",
    "Z = df_w[instr].values           # cloud instrument\n",
    "X = X_controls.values            # nonlinear controls\n",
    "\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "# storage for cross-fitted residuals\n",
    "Y_res = np.zeros_like(Y)\n",
    "D_res = np.zeros_like(D)\n",
    "\n",
    "# =========================================================\n",
    "# XGBoost Model (stable for DML)\n",
    "# =========================================================\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=10,\n",
    "    objective='reg:squarederror',\n",
    "    reg_alpha=1.0,       # L1 penalty\n",
    "    reg_lambda=1.0,      # L2 penalty\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # --- ML: predict E[Y | X] ---\n",
    "    xgb.fit(X[train_idx], Y[train_idx])\n",
    "    Y_res[test_idx] = Y[test_idx] - xgb.predict(X[test_idx])\n",
    "\n",
    "    # --- ML: predict E[D | X] ---\n",
    "    xgb.fit(X[train_idx], D[train_idx])\n",
    "    D_res[test_idx] = D[test_idx] - xgb.predict(X[test_idx])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. DML-IV: LINEAR IV ON RESIDUALs\n",
    "# =========================================================\n",
    "\n",
    "# FIRST STAGE: D_res ~ Z\n",
    "X_first = add_constant(Z)\n",
    "first_stage = OLS(D_res, X_first).fit()\n",
    "D_hat = first_stage.fittedvalues\n",
    "print(first_stage.summary())\n",
    "\n",
    "# SECOND STAGE: Y_res ~ D_hat\n",
    "X_second = add_constant(D_hat)\n",
    "second_stage = OLS(Y_res, X_second).fit()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. Clustered SE\n",
    "# =========================================================\n",
    "\n",
    "clusters = df[[\"muni_code\",\"cl_microYear\"]].copy()\n",
    "clusters[\"muni_code\"] = clusters[\"muni_code\"].astype(int)\n",
    "clusters[\"cl_microYear\"] = clusters[\"cl_microYear\"].astype(\"category\").cat.codes.astype(int)\n",
    "\n",
    "ss_cluster = second_stage.get_robustcov_results(\n",
    "    cov_type=\"cluster\",\n",
    "    groups=clusters\n",
    ")\n",
    "\n",
    "print(\"\\n========== DML-IV (Shallow NN) with Clustered SE ==========\")\n",
    "print(ss_cluster.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "87127d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_hat (DML-IV): -0.057451\n",
      "Two-Way Clustered DML SE: 0.062184\n",
      "95% CI (two-way cluster): (-0.179329, 0.064427)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# from second-stage OLS on residuals\n",
    "theta_hat = second_stage.params[1]\n",
    "theta_hat\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. Required objects\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# cluster IDs as 1D numpy arrays\n",
    "cluster1 = np.asarray(clusters[\"muni_code\"].values)\n",
    "cluster2 = np.asarray(clusters[\"cl_microYear\"].values)\n",
    "\n",
    "# orthogonal scores: ψ_i = Z_res * (Y_res - theta_hat * D_res)\n",
    "psi = Z * (Y_res - theta_hat * D_res)\n",
    "\n",
    "# Jacobian:  J = -(1/n) * Σ Z_res * D_res\n",
    "J = -np.mean(Z * D_res)\n",
    "\n",
    "n = len(psi)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Compute cluster sums of ψ_i\n",
    "# ------------------------------------------------------------\n",
    "def cluster_sum(psi_vec, cluster_id_vec):\n",
    "    cluster_id_vec = np.asarray(cluster_id_vec)\n",
    "    out = {}\n",
    "    for cid in np.unique(cluster_id_vec):\n",
    "        mask = (cluster_id_vec == cid)\n",
    "        out[cid] = psi_vec[mask].sum()\n",
    "    return out\n",
    "\n",
    "# cluster 1 sums\n",
    "C1 = cluster_sum(psi, cluster1)\n",
    "# cluster 2 sums\n",
    "C2 = cluster_sum(psi, cluster2)\n",
    "\n",
    "# intersection clusters: create a single combined cluster label\n",
    "cluster12 = np.array([f\"{a}_{b}\" for a, b in zip(cluster1, cluster2)])\n",
    "C12 = cluster_sum(psi, cluster12)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Variance components\n",
    "# ------------------------------------------------------------\n",
    "V1  = sum(v**2 for v in C1.values())\n",
    "V2  = sum(v**2 for v in C2.values())\n",
    "V12 = sum(v**2 for v in C12.values())\n",
    "\n",
    "# Two-way cluster variance (Cameron–Gelbach–Miller)\n",
    "cluster_var = (V1 + V2 - V12) / (n**2)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Delta method for θ (Jacobian scaling)\n",
    "# ------------------------------------------------------------\n",
    "Var_theta = cluster_var / (J**2)\n",
    "se_dml_tw = np.sqrt(Var_theta)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. CI\n",
    "# ------------------------------------------------------------\n",
    "z_95 = norm.ppf(0.975)\n",
    "ci_low = theta_hat - z_95 * se_dml_tw\n",
    "ci_high = theta_hat + z_95 * se_dml_tw\n",
    "\n",
    "print(f\"Theta_hat (DML-IV): {theta_hat:.6f}\")\n",
    "print(f\"Two-Way Clustered DML SE: {se_dml_tw:.6f}\")\n",
    "print(f\"95% CI (two-way cluster): ({ci_low:.6f}, {ci_high:.6f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e09d48",
   "metadata": {},
   "source": [
    "### 6. Flexible DML-IV (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                    0.6546\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):              0.419\n",
      "Time:                        16:29:16   Log-Likelihood:                -4851.0\n",
      "No. Observations:                5210   AIC:                             9706.\n",
      "Df Residuals:                    5208   BIC:                             9719.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0052      0.009     -0.612      0.541      -0.022       0.011\n",
      "x1             0.0038      0.005      0.809      0.419      -0.005       0.013\n",
      "==============================================================================\n",
      "Omnibus:                     6194.455   Durbin-Watson:                   2.201\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1774593.436\n",
      "Skew:                           5.931   Prob(JB):                         0.00\n",
      "Kurtosis:                      92.633   Cond. No.                         1.80\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# FLEXIBLE PARTIALLY LINEAR IV (RF version)\n",
    "# =========================================================\n",
    "\n",
    "Y = df_w[y_var].values\n",
    "D = df_w[endog].values\n",
    "Z = df_w[instr].values.reshape(-1,1)   # must be 2D for concatenation\n",
    "X = X_controls.values\n",
    "\n",
    "XZ = np.hstack([Z, X])   # for gamma^(3)\n",
    "\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "Y_res = np.zeros_like(Y)\n",
    "D_res = np.zeros_like(D)\n",
    "instr_opt = np.zeros_like(D)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # gamma^(1): E[Y|X]\n",
    "    rf.fit(X[train_idx], Y[train_idx])\n",
    "    Y_res[test_idx] = Y[test_idx] - rf.predict(X[test_idx])\n",
    "\n",
    "    # gamma^(2): E[D|X]\n",
    "    rf.fit(X[train_idx], D[train_idx])\n",
    "    D_res[test_idx] = D[test_idx] - rf.predict(X[test_idx])\n",
    "\n",
    "    # gamma^(3): E[D|Z,X]  <-- flexible IV\n",
    "    rf.fit(XZ[train_idx], D[train_idx])\n",
    "    instr_opt[test_idx] = rf.predict(XZ[test_idx])\n",
    "    \n",
    "\n",
    "# FIRST STAGE: D_res ~ instr_opt\n",
    "X_first = add_constant(instr_opt)\n",
    "first_stage = OLS(D_res, X_first).fit()\n",
    "D_hat = first_stage.fittedvalues\n",
    "print(first_stage.summary())\n",
    "\n",
    "# SECOND STAGE: Y_res ~ D_hat\n",
    "X_second = add_constant(D_hat)\n",
    "second_stage = OLS(Y_res, X_second).fit()\n",
    "\n",
    "print(second_stage.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3a305f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.010\n",
      "Model:                            OLS   Adj. R-squared:                  0.010\n",
      "Method:                 Least Squares   F-statistic:                     53.43\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):           3.08e-13\n",
      "Time:                        16:43:06   Log-Likelihood:                -22367.\n",
      "No. Observations:                5210   AIC:                         4.474e+04\n",
      "Df Residuals:                    5208   BIC:                         4.475e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0326      0.245     -0.133      0.894      -0.514       0.449\n",
      "x1            -0.3803      0.052     -7.310      0.000      -0.482      -0.278\n",
      "==============================================================================\n",
      "Omnibus:                     5493.457   Durbin-Watson:                   1.615\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2396885.477\n",
      "Skew:                           4.569   Prob(JB):                         0.00\n",
      "Kurtosis:                     107.680   Cond. No.                         4.72\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(first_stage.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9598c",
   "metadata": {},
   "source": [
    "### 7. Partially DML-IV (RF) - without weather variables in first stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f22569e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PL-RF FIRST STAGE (restricted X) =====\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.001\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                     2.767\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):             0.0963\n",
      "Time:                        22:49:26   Log-Likelihood:                -22393.\n",
      "No. Observations:                5210   AIC:                         4.479e+04\n",
      "Df Residuals:                    5208   BIC:                         4.480e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0507      0.247     -0.206      0.837      -0.534       0.433\n",
      "x1            -3.1966      1.922     -1.663      0.096      -6.964       0.571\n",
      "==============================================================================\n",
      "Omnibus:                     5293.421   Durbin-Watson:                   1.619\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1998861.232\n",
      "Skew:                           4.307   Prob(JB):                         0.00\n",
      "Kurtosis:                      98.570   Cond. No.                         7.79\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== PL-RF SECOND STAGE (restricted X) =====\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.001\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                     3.117\n",
      "Date:                Mon, 01 Dec 2025   Prob (F-statistic):             0.0775\n",
      "Time:                        22:49:26   Log-Likelihood:                -4990.0\n",
      "No. Observations:                5210   AIC:                             9984.\n",
      "Df Residuals:                    5208   BIC:                             9997.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0081      0.009     -0.918      0.359      -0.025       0.009\n",
      "x1            -0.0376      0.021     -1.765      0.078      -0.079       0.004\n",
      "==============================================================================\n",
      "Omnibus:                     6357.942   Durbin-Watson:                   2.225\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2132867.717\n",
      "Skew:                           6.172   Prob(JB):                         0.00\n",
      "Kurtosis:                     101.350   Cond. No.                         2.44\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Basic variables (demeaned)\n",
    "# ---------------------------------------------------------\n",
    "Y = df_w[y_var].values\n",
    "D = df_w[endog].values\n",
    "Z = df_w[instr].values.reshape(-1,1)\n",
    "X_full = X_controls.values\n",
    "X_cols = X_controls.columns.tolist()\n",
    "\n",
    "restricted_cols = [\n",
    "    c for c in X_cols\n",
    "    if (\"weather\" not in c.lower())\n",
    "    and (\"temp\" not in c.lower())\n",
    "    and (\"rain\" not in c.lower())\n",
    "]\n",
    "\n",
    "X_restricted = X_controls[restricted_cols].values\n",
    "\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "Y_res = np.zeros_like(Y)\n",
    "D_res = np.zeros_like(D)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_full):\n",
    "\n",
    "    # gamma^(1): E[Y|X_full]\n",
    "    rf.fit(X_full[train_idx], Y[train_idx])\n",
    "    Y_res[test_idx] = Y[test_idx] - rf.predict(X_full[test_idx])\n",
    "\n",
    "    # gamma^(2): E[D|X_restricted]\n",
    "    rf.fit(X_restricted[train_idx], D[train_idx])\n",
    "    D_res[test_idx] = D[test_idx] - rf.predict(X_restricted[test_idx])\n",
    "\n",
    "X_first = add_constant(Z)\n",
    "first_stage = OLS(D_res, X_first).fit()\n",
    "D_hat = first_stage.fittedvalues\n",
    "\n",
    "print(\"\\n===== PL-RF FIRST STAGE (restricted X) =====\")\n",
    "print(first_stage.summary())\n",
    "\n",
    "X_second = add_constant(D_hat)\n",
    "second_stage = OLS(Y_res, X_second).fit()\n",
    "\n",
    "print(\"\\n===== PL-RF SECOND STAGE (restricted X) =====\")\n",
    "print(second_stage.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "059ae4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_hat: -0.037599444636163395\n",
      "Jacobian J: 7.329473661415485e-21\n",
      "DML Standard Error (no clustering): 2.222582217788821e+17\n",
      "95% CI: (-4.356181099545248e+17, 4.356181099545248e+17)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# INPUTS YOU ALREADY HAVE:\n",
    "#  - theta_hat      (second-stage coefficient)\n",
    "#  - Y_res, D_res   (partialled-out residuals)\n",
    "#  - Z              (the raw instrument, NOT residualized)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Orthogonal score: ψ_i = Z_i * (Y_res_i - θ * D_res_i)\n",
    "psi = Z * (Y_res - theta_hat * D_res)\n",
    "\n",
    "# Jacobian:  J = -(1/n) * Σ Z_i * D_res_i\n",
    "J = -np.mean(Z * D_res)\n",
    "\n",
    "n = len(psi)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Variance of ψ_i\n",
    "# ------------------------------------------------------------\n",
    "Var_psi = np.mean((psi - psi.mean())**2)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Delta-method variance for θ\n",
    "# Var(θ) = Var(ψ_i) / (n * J^2)\n",
    "# ------------------------------------------------------------\n",
    "Var_theta = Var_psi / (n * (J**2))\n",
    "se_theta = np.sqrt(Var_theta)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CI\n",
    "# ------------------------------------------------------------\n",
    "z_95 = norm.ppf(0.975)\n",
    "ci_low = theta_hat - z_95 * se_theta\n",
    "ci_high = theta_hat + z_95 * se_theta\n",
    "\n",
    "print(\"Theta_hat:\", theta_hat)\n",
    "print(\"Jacobian J:\", J)\n",
    "print(\"DML Standard Error (no clustering):\", se_theta)\n",
    "print(\"95% CI:\", (ci_low, ci_high))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108d024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_hat (DML-IV): -0.037599\n",
      "Two-Way Clustered DML SE: 25867468582248288256.000000\n",
      "95% CI (two-way cluster): (-50699306792428019712.000000, 50699306792428019712.000000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# from second-stage OLS on residuals\n",
    "theta_hat = second_stage.params[1]\n",
    "theta_hat\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. Required objects\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# cluster IDs as 1D numpy arrays\n",
    "cluster1 = np.asarray(clusters[\"muni_code\"].values)\n",
    "cluster2 = np.asarray(clusters[\"cl_microYear\"].values)\n",
    "\n",
    "# orthogonal scores: ψ_i = Z_res * (Y_res - theta_hat * D_res)\n",
    "psi = Z * (Y_res - theta_hat * D_res)\n",
    "\n",
    "# Jacobian:  J = -(1/n) * Σ Z_res * D_res\n",
    "J = -np.mean(Z * D_res)\n",
    "\n",
    "n = len(psi)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Compute cluster sums of ψ_i\n",
    "# ------------------------------------------------------------\n",
    "def cluster_sum(psi_vec, cluster_id_vec):\n",
    "    cluster_id_vec = np.asarray(cluster_id_vec)\n",
    "    out = {}\n",
    "    for cid in np.unique(cluster_id_vec):\n",
    "        mask = (cluster_id_vec == cid)\n",
    "        out[cid] = psi_vec[mask].sum()\n",
    "    return out\n",
    "\n",
    "# cluster 1 sums\n",
    "C1 = cluster_sum(psi, cluster1)\n",
    "# cluster 2 sums\n",
    "C2 = cluster_sum(psi, cluster2)\n",
    "\n",
    "# intersection clusters: create a single combined cluster label\n",
    "cluster12 = np.array([f\"{a}_{b}\" for a, b in zip(cluster1, cluster2)])\n",
    "C12 = cluster_sum(psi, cluster12)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Variance components\n",
    "# ------------------------------------------------------------\n",
    "V1  = sum(v**2 for v in C1.values())\n",
    "V2  = sum(v**2 for v in C2.values())\n",
    "V12 = sum(v**2 for v in C12.values())\n",
    "\n",
    "# Two-way cluster variance (Cameron–Gelbach–Miller)\n",
    "cluster_var = (V1 + V2 - V12) / (n**2)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Delta method for θ (Jacobian scaling)\n",
    "# ------------------------------------------------------------\n",
    "Var_theta = cluster_var / (J**2)\n",
    "se_dml_tw = np.sqrt(Var_theta)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. CI\n",
    "# ------------------------------------------------------------\n",
    "z_95 = norm.ppf(0.975)\n",
    "ci_low = theta_hat - z_95 * se_dml_tw\n",
    "ci_high = theta_hat + z_95 * se_dml_tw\n",
    "\n",
    "print(f\"Theta_hat (DML-IV): {theta_hat:.6f}\")\n",
    "print(f\"Two-Way Clustered DML SE: {se_dml_tw:.6f}\")\n",
    "print(f\"95% CI (two-way cluster): ({ci_low:.6f}, {ci_high:.6f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
